[
  {
    "objectID": "00-content.html",
    "href": "00-content.html",
    "title": "Welcome Back to R",
    "section": "",
    "text": "As noted in the syllabus, your readings will be assigned each week in this area. For this initial week, please read the course content. Read closely the following:\n\nThe syllabus, content, examples, and labs pages for this class.\nThis page. Yes, the whole thing.\n\n\n\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nHow does this course work?\nDo you remember anything about R?\nWhat are the different data types in R?\nHow do you index specific elements of a vector? Why might you want to do that?"
  },
  {
    "objectID": "00-content.html#starting-point-for-this-course",
    "href": "00-content.html#starting-point-for-this-course",
    "title": "Welcome Back to R",
    "section": "Starting point for this course",
    "text": "Starting point for this course\nBetter utilizing existing data can improve our predictive power whilst providing interpretable outputs for considering new policies.\nWARNING: Causation is tough and we will spend the entire course warning you to avoid making causal claims!\n\nNon-Social Science Approaches to Statistical Learning\nA Brief History\nSuppose you are a researcher and you want to teach a computer to recognize images of a tree.\nNote: this is an ``easy” problem. If you show pictures to a 3-year-old, that child will probably be able to tell you if there is a tree in the picture.\nComputer scientists spent about 20 years on this problem because they thought about the problem like nerds and tried to write down a series of rules.\nRules are difficult to form, and simply writing rules misses the key insight: the data can tell you something.\n\n\nSocial Science Approaches to Statistical Learning\nA Brief History\nSuppose you are a researcher and you want to know whether prisons reduce crime.\nfrom “A Call for a Moratorium on Prison Building” (1976)\n\nBetween 1955 and 1975, fifteen states increased the collective capacity of their adult prison systems by 56% (from, on average, 63,100 to 98,649).\nFifteen other states increased capacity by less than 4% (from 49,575 to 51,440).\nIn “heavy-construction” states the crime rate increased by 167%; in “low-construction” states the crime rate increased by 145%.\n\n\n\n\n\nPrison Capacity\nCrime Rate\n\n\n\n\nHigh construction\n\\(\\uparrow\\)~56%\n\\(\\uparrow\\)~167%\n\n\nLow construction\n\\(\\uparrow\\)~4%\n\\(\\uparrow\\)~145%\n\n\n\n\n\nThe Pros and Cons of Correlation\nPros:\n\nNature gives you correlations for free.\nIn principle, everyone can agree on the facts.\n\nCons:\n\nCorrelations are not very helpful.\nThey show what has happened, but not why.\nFor many things, we care about why. The social science perspective asks “why?”\n\n\nWhy a Correlation Exists Between X and Y\n\n\\(X \\rightarrow Y\\) X causes Y (causality)\n\\(X \\leftarrow Y\\) Y causes X (reverse causality)\n\\(Z \\rightarrow X\\); \\(Z \\rightarrow Y\\) Z causes X and Y (common cause)\n\\(X \\rightarrow Y\\); \\(Y \\rightarrow X\\) X causes Y and Y causes X (simultaneous equations)\n\n\n\nUniting Social Science and Computer Science\nWe will start in this course by examining situations where we do not care about why something has happened, but instead we care about our ability to predict its occurrence from existing data.\n(But of course keep in back of mind that if you are making policy, you must care about why something happened).\nWe will also borrow a few other ideas from CS:\n\nAnything is data\n\nSatellite data\nUnstructured text or audio\nFacial expressions or vocal intonations\n\nSubtle improvements on existing techniques\nAn eye towards practical implementability over “cleanliness”\n\n\n\n\nA Case Study in Prediction\nExample: a firm wishes to predict user behavior based on previous purchases or interactions.\nSmall margins \\(\\rightarrow\\) huge payoffs when scaled up.\n\\(.01\\% \\rightarrow\\) $10 million.\nNot obvious why this was true for Netflix; quite obvious why this is true in financial markets.\nFrom a computer science perspective, it only matters that you get that improvement ($$). From a social science perspective, we would want to use the predictions to learn more about why.\n\n\nMore Recent Examples of Prediction\n\nIdentify the risk factors for prostate cancer.\nClassify a tissue sample into one of several cancer classes, based on a gene expression profile.\nClassify a recorded phoneme based on a log-periodogram.\nPredict whether someone will have a heart attack on the basis of demographic, diet and clinical measurements.\nCustomize an email spam detection system.\nIdentify a hand-drawn object.\nDetermine which oscillations of stellar luminosity are likely due to exoplanets.\nIdentify food combinations that cause spikes in blood glucose level for an individual.\nEstablish the relationship between salary and demographic variables in population survey data.\n\n\n\nAn Aside: Nomenclature\nMachine learning arose as a subfield of Artificial Intelligence.\nStatistical learning arose as a subfield of Statistics.\nThere is much overlap; however, a few points of distinction:\n\nMachine learning has a greater emphasis on large scale applications and prediction accuracy.\nStatistical learning emphasizes models and their interpretability, and precision and uncertainty.\n\nBut the distinction has become more and more blurred, and there is a great deal of “cross-fertilization”.\n\n\nObviously true: machine learning has the upper hand in marketing.\n\n\nLearning from Data\nThe following are the basic requirements for statistical learning:\n\nA pattern exists.\nThis pattern is not easily expressed in a closed mathematical form.\nYou have data."
  },
  {
    "objectID": "00-content.html#case-study-1-global-renewable-energy-production",
    "href": "00-content.html#case-study-1-global-renewable-energy-production",
    "title": "Welcome Back to R",
    "section": "Case study 1: Global Renewable Energy Production",
    "text": "Case study 1: Global Renewable Energy Production\nImagine you are evaluating countries for a potential investment in renewable energy. Headlines like “Renewable Energy Capacity Growth Worldwide” have piqued your interest. Reports from various sources show diverse graphs and charts and you’re curious about the underlying data. You want to know which countries are leading the way in renewable energy production and which are lagging behind. You want to know which countries are growing their renewable energy production the fastest. In short: you want to know which countries are the best bets for investment. You might see something like the following:\n\n\n\n\n\nYou might want to look into the underlying data (in this case, fabricated) and think about what to do next. In this sense, you have learned from data."
  },
  {
    "objectID": "00-content.html#case-study-2-us-homicides-by-firearm",
    "href": "00-content.html#case-study-2-us-homicides-by-firearm",
    "title": "Welcome Back to R",
    "section": "Case study 2: US homicides by firearm",
    "text": "Case study 2: US homicides by firearm\nImagine you live in Europe (if only!) and are offered a job in a US company with many locations in every state. It is a great job, but headlines such as US Gun Homicide Rate Higher Than Other Developed Countries1 have you worried. Fox News runs a scary looking graphic, and charts like the one below only add to you anxiety:\n\n\n\n\n\n\nOr even worse, this version from everytown.org:\n\n\n\n\n\n\nBut then you remember that (1) this is a hypothetical exercise; (2) you’ll take literally any job at this point; and (3) Geographic diversity matters – the United States is a large and diverse country with 50 very different states (plus the District of Columbia and some lovely territories).2\n\n\n\n\n\nCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial.\n\n\n\nThis is a relatively simple and straightforward problem in social science: you have options of where to live, and want to determine the safety of the various states. Your “research” is clearly policy-relevant: you will eventually have to live somewhere. In this course, we will begin to tackle the problem by examining data related to gun homicides in the US during 2010 using R as a motivating example along the way.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Ideally, this is a refresher. However, we are aware that your preparation in previously courses varies greatly from student to student. Moreover, we want you to be aware that the usefulness of some of these early building blocks may not be immediately obvious. Later in the class you will appreciate having these skills. Mastery will be rewarded both in this class and (of course) in life.\n\nThe Pre-Basics\nWe’ve now covered a short bit of material. The remainder of this first lecture will be covering setting up R and describing some common errors."
  },
  {
    "objectID": "01-content.html",
    "href": "01-content.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "This page.\nChapter 1 of Introduction to Statistical Learning, available here.\nOptional: The “Tidy Your Data” tutorial on Rstudio Cloud Primers"
  },
  {
    "objectID": "01-content.html#some-reminders",
    "href": "01-content.html#some-reminders",
    "title": "Introduction to the tidyverse",
    "section": "Some Reminders:",
    "text": "Some Reminders:\n\nStart labs early!\n\nThey are not trivial.\nThey are not short.\nThey are not easy.\nThey are not optional.\n\nYou install.packages(\"packageName\") once on your computer.\n\nAnd never ever ever in your code.\n\nYou load an already-installed package using library(packageName) in a code chunk\n\nNever in your console\nWhen RMarkdown knits, it starts a whole new, empty session that has no knowledge of what you typed into the console\n\nSlack\n\nUse it.\nI would very much prefer posting in the class-visible channels. Others can learn from your issues.\n\nWe have a channel just for labs and R. Please use that one.\n\n\n\n\nGroup Projects\nYour final is a group project. You will also have two “mini” projects. They comprise a large part of your grade. As mentioned last week, this mean that you need to start planning soon.\nTo aid in your planning, here are the required elements of your final project.\n\nYou must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.\nYou must visualize 3 interesting features of that data.\nYou must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.\nYou must think critically about your analysis and be able to identify potential issues.\nYou must present your analysis as if presenting to a C-suite executive.\n\nYour mini-projects along the way will be more structured, but will serve to guide you towards the final project.\n\n\nTeams\nPlease form teams of 3 people. Once all agree to be on a team, have ONE PERSON email our TA Allen scovelpa@msu.edu and cc all of the members of the team so that nobody is surprised to be included on a team. Title the email [SSC442] - Group Formation. Tell us your team name (be creative), and list in the email the names of all of the team members and their email address (in addition to cc-ing those team members on the email).\nIf you opt to not form a team, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with two others from the “willing to be randomly assigned” pool.\nSend this email by January 20th and we will assign un-teamed folks at the beginning of the following week. Project 1 is due in no time. See schedule for all the important project dates.\n\n\nGuiding Question\nFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\nWhy do we want tidy data?\nWhat are the challenges associated with shaping things into a tidy format?"
  },
  {
    "objectID": "01-content.html#tidy-data",
    "href": "01-content.html#tidy-data",
    "title": "Introduction to the tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\n\nWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\n\n\nlibrary(dslabs)\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\n\nlibrary(dslabs)\ndata(\"gapminder\") # gapminder will now be a data.frame in your \"environment\" (memory)\ntidy_data <- gapminder %>%\n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) %>%\n  select(country, year, fertility)\nhead(tidy_data, 6)\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n\n\n      country 1960 1961 1962\n1     Germany 2.41 2.44 2.47\n2 South Korea 6.16 5.99 5.79\n\n\nThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables’ values, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\n\nTRY IT\n\nExamine the built-in dataset co2. Which of the following is true:\n\n\n\nco2 is tidy data: it has one year for each row.\nco2 is not tidy: we need at least one column with a character vector.\nco2 is not tidy: it is a matrix instead of a data frame.\nco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\n\n\n\nExamine the built-in dataset ChickWeight. Which of the following is true:\n\n\n\nChickWeight is not tidy: each chick has more than one row.\nChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\nChickWeight is not tidy: we are missing the year column.\nChickWeight is tidy: it is stored in a data frame.\n\n\n\nExamine the built-in dataset BOD. Which of the following is true:\n\n\n\nBOD is not tidy: it only has six rows.\nBOD is not tidy: the first column is just an index.\nBOD is tidy: each row is an observation with two values (time and demand)\nBOD is tidy: all small datasets are tidy by definition.\n\n\n\nWhich of the following built-in datasets is tidy (you can pick more than one):\n\n\n\nBJsales\nEuStockMarkets\nDNase\nFormaldehyde\nOrange\nUCBAdmissions"
  },
  {
    "objectID": "01-content.html#manipulating-data-frames",
    "href": "01-content.html#manipulating-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Manipulating data frames",
    "text": "Manipulating data frames\nThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\n\nAdding a column with mutate\nWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\n\nlibrary(dslabs)\ndata(\"murders\")\nmurders <- mutate(murders, rate = total / population * 100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\n\nhead(murders)\n\n       state abb region population total     rate\n1    Alabama  AL  South    4779736   135 2.824424\n2     Alaska  AK   West     710231    19 2.675186\n3    Arizona  AZ   West    6392017   232 3.629527\n4   Arkansas  AR  South    2915918    93 3.189390\n5 California  CA   West   37253956  1257 3.374138\n6   Colorado  CO   West    5029196    65 1.292453\n\n\nNote: Although we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\n\nSubsetting with filter\nNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\n\nfilter(murders, rate <= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211\n\n\n\n\nSelecting columns with select\nAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\n\nnew_table <- select(murders, state, region, rate)\nfilter(new_table, rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\n\nTRY IT\n\nLoad the dplyr package and the murders dataset.\n\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(murders)\n\nYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\n\nmurders <- mutate(murders, population_in_millions = population / 10^6)\n\nWe can write population rather than murders$population because mutate is part of dplyr. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders <- [your code]) so we can keep using this variable.\n\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\nWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\n\nselect(murders, state, population) %>% head()\n\nUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\n\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\n\n\nfilter(murders, state == \"New York\")\n\nYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida <- filter(murders, state != \"Florida\")\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\n\n\nfilter(murders, state %in% c(\"New York\", \"Texas\"))\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nfilter(murders, population < 5000000 & region == \"Northeast\")\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank."
  },
  {
    "objectID": "01-content.html#the-pipe",
    "href": "01-content.html#the-pipe",
    "title": "Introduction to the tidyverse",
    "section": "The pipe: %>%",
    "text": "The pipe: %>%\nWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %>%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %>%. The code looks like this:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n\n16 %>% sqrt()\n\n[1] 4\n\n\nWe can continue to pipe values along:\n\n16 %>% sqrt() %>% log2()\n\n[1] 2\n\n\nThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n\n16 %>% sqrt() %>% log(base = 2)\n\n[1] 2\n\n\nTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\n\nmurders %>% select(state, region, rate) %>% filter(rate <= 0.71)\n\nmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe. It’s worth noting that as of R 4.1, there is a base-R version of the pipe |>, though this has its disadvantages. We’ll stick with %>% for now.\n\nTRY IT\n\nThe pipe %>% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\n\n\nmurders <- mutate(murders, rate =  total / population * 100000,\n                  rank = rank(-rate))\n\nIn the solution to the previous exercise, we did the following:\n\nmy_states <- filter(murders, region %in% c(\"Northeast\", \"West\") &\n                      rate < 1)\n\nselect(my_states, state, rate, rank)\n\nThe pipe %>% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\n\nmutate(murders, rate =  total / population * 100000,\n       rank = rank(-rate)) %>%\n  select(state, rate, rank)\n\nNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %>%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %>% to do this in just one line.\n\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %>%. The code should look something like this:\n\n\nmy_states <- murders %>%\n  mutate SOMETHING %>%\n  filter SOMETHING %>%\n  select SOMETHING"
  },
  {
    "objectID": "01-content.html#summarizing-data",
    "href": "01-content.html#summarizing-data",
    "title": "Introduction to the tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\n\n\n\n\nsummarize\nThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\n\nlibrary(dplyr)\nlibrary(dslabs)\ndata(heights)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\n\nThe following code computes the average and standard deviation for females:\n\ns <- heights %>%\n  filter(sex == \"Female\") %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\ns\n\n   average standard_deviation\n1 64.93942           3.760656\n\n\nThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\n\ns$average\n\n[1] 64.93942\n\ns$standard_deviation\n\n[1] 3.760656\n\n\nAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(median = median(height), minimum = min(height),\n            maximum = max(height))\n\n    median minimum maximum\n1 64.98031      51      79\n\n\nWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\n\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In later sections, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\n\nmurders <- murders %>% mutate(rate = total/population*100000)\n\nRemember that the US murder rate is not the average of the state murder rates:\n\nsummarize(murders, mean(rate))\n\n  mean(rate)\n1   2.779125\n\n\nThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000)\nus_murder_rate\n\n      rate\n1 3.034555\n\n\nThis computation counts larger states proportionally to their size which results in a larger value.\n\n\npull\nThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\n\nclass(us_murder_rate)\n\n[1] \"data.frame\"\n\n\nsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\n\nus_murder_rate %>% pull(rate)\n\n[1] 3.034555\n\n\nThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\n\nus_murder_rate <- murders %>%\n  summarize(rate = sum(total) / sum(population) * 100000) %>%\n  pull(rate)\n\nus_murder_rate\n\n[1] 3.034555\n\n\nwhich is now a numeric:\n\nclass(us_murder_rate)\n\n[1] \"numeric\"\n\n\n\n\nGroup then summarize with group_by\nA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\n\nheights %>% group_by(sex)\n\n# A tibble: 1,050 × 2\n# Groups:   sex [2]\n   sex    height\n   <fct>   <dbl>\n 1 Male       75\n 2 Male       70\n 3 Male       68\n 4 Male       74\n 5 Male       61\n 6 Female     65\n 7 Female     66\n 8 Female     62\n 9 Female     66\n10 Male       67\n# ℹ 1,040 more rows\n\n\nThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\n\nheights %>%\n  group_by(sex) %>%\n  summarize(average = mean(height), standard_deviation = sd(height))\n\n# A tibble: 2 × 3\n  sex    average standard_deviation\n  <fct>    <dbl>              <dbl>\n1 Female    64.9               3.76\n2 Male      69.3               3.61\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\n\nmurders %>%\n  group_by(region) %>%\n  summarize(median_rate = median(rate))\n\n# A tibble: 4 × 2\n  region        median_rate\n  <fct>               <dbl>\n1 Northeast            1.80\n2 South                3.40\n3 North Central        1.97\n4 West                 1.29"
  },
  {
    "objectID": "01-content.html#sorting-data-frames",
    "href": "01-content.html#sorting-data-frames",
    "title": "Introduction to the tidyverse",
    "section": "Sorting data frames",
    "text": "Sorting data frames\nWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\n\nmurders %>%\n  arrange(population) %>%\n  head()\n\n                 state abb        region population total       rate\n1              Wyoming  WY          West     563626     5  0.8871131\n2 District of Columbia  DC         South     601723    99 16.4527532\n3              Vermont  VT     Northeast     625741     2  0.3196211\n4         North Dakota  ND North Central     672591     4  0.5947151\n5               Alaska  AK          West     710231    19  2.6751860\n6         South Dakota  SD North Central     814180     8  0.9825837\n\n\nWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\n\nmurders %>%\n  arrange(rate) %>%\n  head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\n\nmurders %>%\n  arrange(desc(rate))\n\n\nNested sorting\nIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\n\nmurders %>%\n  arrange(region, rate) %>%\n  head()\n\n          state abb    region population total      rate\n1       Vermont  VT Northeast     625741     2 0.3196211\n2 New Hampshire  NH Northeast    1316470     5 0.3798036\n3         Maine  ME Northeast    1328361    11 0.8280881\n4  Rhode Island  RI Northeast    1052567    16 1.5200933\n5 Massachusetts  MA Northeast    6547629   118 1.8021791\n6      New York  NY Northeast   19378102   517 2.6679599\n\n\n\n\nThe top \\(n\\)\nIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\n\nmurders %>% top_n(5, rate)\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Maryland  MD         South    5773552   293  5.074866\n4             Missouri  MO North Central    5988927   321  5.359892\n5       South Carolina  SC         South    4625364   207  4.475323\n\n\nNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\n\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\ndata(NHANES)\n\nThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\n\nlibrary(dslabs)\ndata(na_example)\nmean(na_example)\n\n[1] NA\n\nsd(na_example)\n\n[1] NA\n\n\nTo ignore the NAs we can use the na.rm argument:\n\nmean(na_example, na.rm = TRUE)\n\n[1] 2.301754\n\nsd(na_example, na.rm = TRUE)\n\n[1] 1.22338\n\n\nLet’s now explore the NHANES data.\n\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\n\nHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\n\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\nNow report the min and max values for the same group.\nCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\nRepeat exercise 4 for males.\nWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\nFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure."
  },
  {
    "objectID": "01-content.html#tibbles",
    "href": "01-content.html#tibbles",
    "title": "Introduction to the tidyverse",
    "section": "Tibbles",
    "text": "Tibbles\nTidy data must be stored in data frames. We have been using the murders data frame throughout the unit. In an earlier section we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\n\nmurders %>% group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   <chr>                <chr> <fct>          <dbl> <dbl> <dbl>\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followed by dimensions. We can learn the class of the returned object using:\n\nmurders %>% group_by(region) %>% class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\n\nTibbles display better\nThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\n\nSubsets of tibbles are tibbles\nIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\n\nclass(murders[,4])\n\n[1] \"numeric\"\n\n\nis not a data frame. With tibbles this does not happen:\n\nclass(as_tibble(murders)[,4])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\n\nclass(as_tibble(murders)$population)\n\n[1] \"numeric\"\n\n\nA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\n\nmurders$Population\n\nNULL\n\n\nreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\n\nas_tibble(murders)$Population\n\nWarning: Unknown or uninitialised column: `Population`.\n\n\nNULL\n\n\n\n\nTibbles can have complex entries\nWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\n\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\n\n# A tibble: 3 × 2\n     id func  \n  <dbl> <list>\n1     1 <fn>  \n2     2 <fn>  \n3     3 <fn>  \n\n\n\n\nTibbles can be grouped\nThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\n\nCreate a tibble using tibble instead of data.frame\nIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\n\ngrades <- tibble(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\n\nNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90))\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo avoid this, we use the rather cumbersome argument stringsAsFactors:\n\ngrades <- data.frame(names = c(\"John\", \"Juan\", \"Jean\", \"Yao\"),\n                     exam_1 = c(95, 80, 90, 85),\n                     exam_2 = c(90, 85, 85, 90),\n                     stringsAsFactors = FALSE)\nclass(grades$names)\n\n[1] \"character\"\n\n\nTo convert a regular data frame to a tibble, you can use the as_tibble function.\n\nas_tibble(grades) %>% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "01-content.html#the-dot-operator",
    "href": "01-content.html#the-dot-operator",
    "title": "Introduction to the tidyverse",
    "section": "The dot operator",
    "text": "The dot operator\nOne of the advantages of using the pipe %>% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\n\ntab_1 <- filter(murders, region == \"South\")\ntab_2 <- mutate(tab_1, rate = total / population * 10^5)\nrates <- tab_2$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nWe can avoid defining any new intermediate objects by instead typing:\n\nfilter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  summarize(median = median(rate)) %>%\n  pull(median)\n\n[1] 3.398069\n\n\nWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\n\nrates <-   filter(murders, region == \"South\") %>%\n  mutate(rate = total / population * 10^5) %>%\n  .$rate\nmedian(rates)\n\n[1] 3.398069\n\n\nIn the next section, we will see other instances in which using the . is useful."
  },
  {
    "objectID": "01-content.html#do",
    "href": "01-content.html#do",
    "title": "Introduction to the tidyverse",
    "section": "do",
    "text": "do\nThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %>%, tidyverse functions consistently take data frames and return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described earlier. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn the summarize section (above), we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive something unexpected. Prior to R 4.1, we would receive an error. After R 4.1, we actually get:\n\ndata(heights)\nheights %>%\n  filter(sex == \"Female\") %>%\n  summarize(range = quantile(height, c(0, 0.5, 1)))\n\nWe probably wanted three columns: min, median, and max. We can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame. Note that it returns a single-row data frame.\n\nmy_summary <- function(dat){\n  x <- quantile(dat$height, c(0, 0.5, 1))\n  tibble(min = x[1], median = x[2], max = x[3])\n}\n\nWe can now apply the function to the heights dataset to obtain the summaries:\n\nheights %>%\n  group_by(sex) %>%\n  my_summary\n\n# A tibble: 1 × 3\n    min median   max\n  <dbl>  <dbl> <dbl>\n1    50   68.5  82.7\n\n\nBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary(.))\n\n# A tibble: 2 × 4\n# Groups:   sex [2]\n  sex      min median   max\n  <fct>  <dbl>  <dbl> <dbl>\n1 Female    51   65.0  79  \n2 Male      50   69    82.7\n\n\nNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary())\n\nIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\n\nheights %>%\n  group_by(sex) %>%\n  do(my_summary)\n\nSo do serves as a bridge between non-tidyverse functions and the tidyverse."
  },
  {
    "objectID": "01-content.html#the-purrr-package",
    "href": "01-content.html#the-purrr-package",
    "title": "Introduction to the tidyverse",
    "section": "The purrr package",
    "text": "The purrr package\nIn previous sections (and labs) we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  sum(x)\n}\nn <- 1:25\ns_n <- sapply(n, compute_s_n)\ns_n\n\n [1]   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120 136 153 171 190\n[20] 210 231 253 276 300 325\n\n\nThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\n\nlibrary(purrr) # or library(tidyverse)\nn <- 1:25\ns_n <- map(n, compute_s_n)\nclass(s_n)\n\n[1] \"list\"\n\n\nIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\n\ns_n <- map_dbl(n, compute_s_n)\nclass(s_n)\n\n[1] \"numeric\"\n\n\nThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\n\ns_n <- map_df(n, compute_s_n)\n\nWe need to change the function to make this work:\n\ncompute_s_n <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x))\n}\ns_n <- map_df(n, compute_s_n)\nhead(s_n)\n\n# A tibble: 6 × 1\n    sum\n  <int>\n1     1\n2     3\n3     6\n4    10\n5    15\n6    21\n\n\nBecause map_df returns a tibble, we can have more columns defined in our function and returned.\n\ncompute_s_n2 <- function(n){\n  x <- 1:n\n  tibble(sum = sum(x), sumSquared = sum(x^2))\n}\ns_n <- map_df(n, compute_s_n2)\nhead(s_n)\n\n# A tibble: 6 × 2\n    sum sumSquared\n  <int>      <dbl>\n1     1          1\n2     3          5\n3     6         14\n4    10         30\n5    15         55\n6    21         91\n\n\nThe purrr package provides much more functionality not covered here. For more details you can consult this online resource."
  },
  {
    "objectID": "01-content.html#tidyverse-conditionals",
    "href": "01-content.html#tidyverse-conditionals",
    "title": "Introduction to the tidyverse",
    "section": "Tidyverse conditionals",
    "text": "Tidyverse conditionals\nA typical data analysis will often involve one or more conditional operations. In the section on Conditionals, we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\n\ncase_when\nThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\n\nx <- c(-2, -1, 0, 1, 2)\ncase_when(x < 0 ~ \"Negative\",\n          x > 0 ~ \"Positive\",\n          x == 0  ~ \"Zero\")\n\n[1] \"Negative\" \"Negative\" \"Zero\"     \"Positive\" \"Positive\"\n\n\nA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\n\nmurders %>%\n  mutate(group = case_when(\n    abb %in% c(\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\") ~ \"New England\",\n    abb %in% c(\"WA\", \"OR\", \"CA\") ~ \"West Coast\",\n    region == \"South\" ~ \"South\",\n    TRUE ~ \"Other\")) %>%\n  group_by(group) %>%\n  summarize(rate = sum(total) / sum(population) * 10^5)\n\n# A tibble: 4 × 2\n  group        rate\n  <chr>       <dbl>\n1 New England  1.72\n2 Other        2.71\n3 South        3.63\n4 West Coast   2.90\n\n\nThat TRUE on the fourth line of case_when serves as a catch-all. As case_when steps through the conditions, if none of them are true, it comes to the last line. Since TRUE is always true, the function will return “Other”. Leaving out the last line of case_when would result in NA values for any observation that fails the first three conditionals. This may or may not be what you want.\n\n\nbetween\nA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\n\nx >= a & x <= b\n\nHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\n\nbetween(x, a, b)\n\n\nTRY IT\n\nLoad the murders dataset. Which of the following is true?\n\n\n\nmurders is in tidy format and is stored in a tibble.\nmurders is in tidy format and is stored in a data frame.\nmurders is not in tidy format and is stored in a tibble.\nmurders is not in tidy format and is stored in a data frame.\n\n\n\nUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\nUse the group_by function to convert murders into a tibble that is grouped by region.\nWrite tidyverse code that is equivalent to this code:\n\n\nexp(mean(log(murders$population)))\n\nWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %>%.\n\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number."
  },
  {
    "objectID": "02-content.html",
    "href": "02-content.html",
    "title": "Effective Visualizations",
    "section": "",
    "text": "This page.\n\n\\[\\sum_{i=1}^N \\frac{1}{N} x_1\\]"
  },
  {
    "objectID": "02-content.html#a-starting-list-from-tufte",
    "href": "02-content.html#a-starting-list-from-tufte",
    "title": "Effective Visualizations",
    "section": "A Starting List (from Tufte)",
    "text": "A Starting List (from Tufte)\n\nThe representation of numbers, as physically measured on the surface of the graph itself, should be directly proportional to the numerical quantities represented.\nClear, detailed and thorough labeling should be used to defeat graphical distortion and ambiguity. Write out explanations of the data on the graph itself. Label important events in the data.\nShow data variation, not design variation.\nIn time-series displays of money, deflated and standardized units of monetary measurement are nearly always better than nominal units.\nThe number of information carrying (variable) dimensions depicted should not exceed the number of dimensions in the data. Graphics must not quote data out of context.\n\n\nExamples (Dos and Don’ts)\nAs with the discussion above, we will be using these libraries—note the addition of gridExtra:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(gridExtra)\n\n\n\nEncoding data using visual cues\nVisual cues are any element of design that gives the viewer clues as to how to use the object. For instance, we can look at door handles like these\n\nand know exactly what to do with them. We don’t even need the “PUSH” and “PULL” – approaching just the pull handle in the wild gives sufficient visual cues that you know the door is a “pull” door. Encountering a metal plate with no handle is going to imply “push”. If the door on the right were a “push” door, you’d be momentarily confused! It’s poor design. Your plots should use visual cues to help readers understand how to use them with no confusion.\nWe start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue.\n\n\n\nTo illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:\n\n\n\n\n\n\n\n\n\nHere we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. The donut chart is an example of a plot that uses only area:\n\n\n\n\n\n\n\n\n\nTo see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In fact, the pie R function help file states that:\n\nPie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.\n\nIn this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy:\n\n\n\n\n \n  \n    Browser \n    2000 \n    2015 \n  \n \n\n  \n    Opera \n    3 \n    2 \n  \n  \n    Safari \n    21 \n    22 \n  \n  \n    Firefox \n    23 \n    21 \n  \n  \n    Chrome \n    26 \n    29 \n  \n  \n    IE \n    28 \n    27 \n  \n\n\n\n\n\nThe preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.\n\n\n\n\n\n\n\n\n\nNotice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.\nIf for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:\n\n\n\n\n\n\n\n\n\nIn general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once.\n\n\nAvoid pseudo-three-dimensional plots\nThe figure below, taken from the scientific literature5, shows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.\n (Image courtesy of Karl Broman)\nHumans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:\n\n##First read data\nurl <- \"https://github.com/kbroman/Talk_Graphs/raw/master/R/fig8dat.csv\"\ndat <- read.csv(url)\n\n##Now make alternative plot\ndat %>% gather(drug, survival, -log.dose) %>%\n  mutate(drug = gsub(\"Drug.\",\"\",drug)) %>%\n  ggplot(aes(log.dose, survival, color = drug)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNotice how much easier it is to determine the survival values.\nPseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:\n  (Images courtesy of Karl Broman)\n\n\nAvoid too many significant digits\nBy default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:\n\n\n\n\n \n  \n    state \n    year \n    Measles \n    Pertussis \n    Polio \n  \n \n\n  \n    California \n    1940 \n    37.8826320 \n    18.3397861 \n    0.8266512 \n  \n  \n    California \n    1950 \n    13.9124205 \n    4.7467350 \n    1.9742639 \n  \n  \n    California \n    1960 \n    14.1386471 \n    NA \n    0.2640419 \n  \n  \n    California \n    1970 \n    0.9767889 \n    NA \n    NA \n  \n  \n    California \n    1980 \n    0.3743467 \n    0.0515466 \n    NA \n  \n\n\n\n\n\nWe are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:\n\n\n\n\n \n  \n    state \n    year \n    Measles \n    Pertussis \n    Polio \n  \n \n\n  \n    California \n    1940 \n    37.9 \n    18.3 \n    0.8 \n  \n  \n    California \n    1950 \n    13.9 \n    4.7 \n    2.0 \n  \n  \n    California \n    1960 \n    14.1 \n    NA \n    0.3 \n  \n  \n    California \n    1970 \n    1.0 \n    NA \n    NA \n  \n  \n    California \n    1980 \n    0.4 \n    0.1 \n    NA \n  \n\n\n\n\n\nUseful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3).\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:\n\n\n\n\n \n  \n    state \n    disease \n    1940 \n    1950 \n    1960 \n    1970 \n    1980 \n  \n \n\n  \n    California \n    Measles \n    37.9 \n    13.9 \n    14.1 \n    1 \n    0.4 \n  \n  \n    California \n    Pertussis \n    18.3 \n    4.7 \n    NA \n    NA \n    0.1 \n  \n  \n    California \n    Polio \n    0.8 \n    2.0 \n    0.3 \n    NA \n    NA \n  \n\n\n\n\n\n\n\nKnow your audience\nGraphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot.\nAs a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest.\n\n\nKnow when to include 0\nWhen using barplots, it is misinformative not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example used by Peter Aldhous in this lecture: http://paldhous.github.io/ucb/2016/dataviz/week2.html.\n (Source: Fox News, via Media Matters6.)\nFrom the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:\n\n\n\n\n\n\n\n\n\nHere is another example, described in detail in a Flowing Data blog post: \n(Source: Fox News, via Flowing Data7.)\nThis plot makes a 13% increase look like a five fold change. Here is the appropriate plot:\n\n\n\n\n\n\n\n\n\nFinally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change: \n(Source: Venezolana de Televisión via Pakistan Today8 and Diego Mariano.)\nHere is the appropriate plot:\n\n\n\n\n\n\n\n\n\nWhen using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:\n\n\n\n\n\n\n\n\n\nNote that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability.\n\n\nDo not distort quantities\nDuring President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations: \n(Source: The 2011 State of the Union Address9)\nJudging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:\n\ngdp <- c(14.6, 5.7, 5.3, 3.3, 2.5)\ngdp_data <- data.frame(Country = rep(c(\"United States\", \"China\", \"Japan\", \"Germany\", \"France\"),2),\n           y = factor(rep(c(\"Radius\",\"Area\"),each=5), levels = c(\"Radius\", \"Area\")),\n           GDP= c(gdp^2/min(gdp^2), gdp/min(gdp))) %>%\n   mutate(Country = reorder(Country, GDP))\ngdp_data %>%\n  ggplot(aes(Country, y, size = GDP)) +\n  geom_point(show.legend = FALSE, color = \"blue\") +\n  scale_size(range = c(2,25)) +\n  coord_flip() + \n  ylab(\"\") + xlab(\"\") # identical to labs(y = \"\", x = \"\")\n\n\n\n\n\n\n\n\nNot surprisingly, ggplot2 defaults to using area rather than radius. Of course, in this case, we really should not be using area at all since we can use position and length:\n\ngdp_data %>%\n  filter(y == \"Area\") %>%\n  ggplot(aes(Country, GDP)) +\n  geom_bar(stat = \"identity\", width = 0.5) +\n  labs(y = \"GDP in trillions of US dollars\")\n\n\n\n\n\n\n\n\n\n\nOrder categories by a meaningful value\nWhen one of the axes is used to show categories, as is done in barplots, the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.\nWe previously learned how to use the reorder function, which helps us achieve this goal. To appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate:\n\ndata(murders)\np1 <- murders %>% mutate(murder_rate = total / population * 100000) %>%\n  ggplot(aes(x = state, y = murder_rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(size = 8))  +\n  xlab(\"\")\n\np2 <- murders %>% mutate(murder_rate = total / population * 100000) %>%\n  mutate(state = reorder(state, murder_rate)) %>% # here's the magic!\n  ggplot(aes(x = state, y = murder_rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(size = 8))  +\n  xlab(\"\")\n\ngrid.arrange(p1, p2, ncol = 2) # we'll cover this later\n\n\n\n\n\n\n\n\nWe can make the second plot like this:\n\n\n\nThe reorder function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other:\n\n\n\n\n\n\n\n\n\nThe first orders the regions alphabetically, while the second orders them by the group’s median."
  },
  {
    "objectID": "02-content.html#show-the-data",
    "href": "02-content.html#show-the-data",
    "title": "Effective Visualizations",
    "section": "Show the data",
    "text": "Show the data\nWe have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups.\nTo motivate our first principle, “show the data”, we go back to our artificial example of describing heights to a person who is unaware of some basic facts about the population of interest (and is otherwise unsophisticated). This time let’s assume that this person is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors.10 The plot looks like this:\n\n\n\n\n\n\n\n\n\nThe average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.\nThis brings us to our first principle: show the data. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points:\n\n\n\n\n\n\n\n\n\nFor example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.\nThe first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:\n\nheights %>%\n  ggplot(aes(sex, height)) +\n  geom_jitter(width = 0.1, alpha = 0.2)\n\n\n\n\n\n\n\n\nNow we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer."
  },
  {
    "objectID": "02-content.html#faceting",
    "href": "02-content.html#faceting",
    "title": "Effective Visualizations",
    "section": "Faceting",
    "text": "Faceting\nLooking at the previous plot, it’s easy to tell that males tend to be taller than females. Before, we showed how we can plot two distributions over each other using an aesthetic mapping. Something like this:\n\nheights %>%\n  ggplot(aes(x = height, fill = sex)) +\n  geom_histogram(alpha = .5, show.legend = TRUE) +\n  labs(fill = 'Sex')\n\n\n\n\n\n\n\n\nSometimes, putting the plots on top of each other, even with a well-chosen alpha, does not clearly communicate the differences in the distribution. When we want to compare side-by-side, we will often use facets. Facets are a bit like supercharged aesthetic mapping because they let us separate plots based on categorical variables, but instead of putting them together, we can have side-by-side plots.\nTwo functions in ggplot give facets: facet_wrap and facet_grid. We’ll use facet_grid as this is a little more powerful.\nFacets are added as an additional layer like this: + facet_grid(. ~ sex). Inside the function, we have a “formula” that is written without quotes (which is unusual for R). Since facet_grid takes a “formula”, all we have to do to facet is decide how we want to lay out our plots. If we want each of the faceting groups to lie along the vertical axis, we put the variable on which we want to facet before the “~”, and after the “~” we simply put a period. If we want the groups to lie along the horizontal axis, we put the variable after the “~” and the period before. In the example, we’ll separate the histogram by drawing them side by side along the horizontal axis.\n\nheights %>%\n  ggplot(aes(x = height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(.~sex)\n\n\n\n\n\n\n\n\nThis would be the result if we took the females, plotted the histogram, then took the males, made another histogram, and then put them side by side. But we do it in one command by adding +facet_grid(...)\n\nUse common axes with facets\nSince we have plots side-by-side, they can have different scales along the x-axis (or along the y-axis if we were stacking with sex ~ .). We want to be careful here - if we don’t have matching scales on these axes, then it’ll be really hard to visually see differences in the distribution.\nAs an example of what not to do, and to show that we can use the scales argument in facet_grid, we can allow the x-axis to freely scale between the plots. This makes it hard to tell that males are, on average, taller because the average male height, despite being larger than the average female height (70 vs. 65 or so) falls in the same location within the plot box. Note that 80 is the extreme edge for the left plot, but not in the right plot.\n\nheights %>%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(. ~ sex, scales = \"free_x\")\n\n\n\n\n\n\n\n\n\n\nAlign plots vertically to see horizontal changes and horizontally to see vertical changes\nIn these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:\n\nheights %>%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(. ~ sex)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis plot makes it much easier to notice that men’s heights are, on average, higher.\nThe sample size of females is smaller than of males – that is, we have more males in the data. Try table(heights$sex) to see this. It’s also clear from the above plot because the height of the bars on the y-axis (count) are smaller for females. If we are interested in the distribution within our sample, this is useful. If we’re interested in the distribution of females vs. the distribution of males, we might want to re-scale the y-axis. Here we use scales = 'free_y' to allow each of the y-axes to have their own scale. Pay close attention to the axis labels now!\n\np2 <- heights %>%\n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(sex~., scales = 'free_y')\np2\n\n\n\n\n\n\n\n\nWe still have count on the y-axis, so we didn’t switch to density (though it would look the same). Instead, we rescaled the y-axis, which gives us a different perspective but still contains the count information.\nIf we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points:\n\np3=heights %>%\n  ggplot(aes(sex, height)) +\n  geom_boxplot(coef=3) +\n  geom_jitter(width = 0.1, alpha = 0.2) +\n  ylab(\"Height in inches\")\n\np3\n\n\n\n\n\n\n\n\nNow contrast and compare these three plots, based on exactly the same data:\n\n\n\n\n\n\n\n\n\nNotice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions.\n\n\nFacet grids\nAs the name implies, facet_grid can make more than just side-by-plots. If we specify variables on boths sides of the “~”, we get a grid of plots.\n\ngapminder::gapminder %>%\n  filter(year %in% c(1952,1972, 1992, 2002)) %>%\n  filter(continent != 'Oceania') %>%\n  ggplot(aes(x = lifeExp)) + \n  geom_density() +\n  facet_grid(continent ~ year)\n\n\n\n\n\n\n\n\nThis makes it easy to read the life expectancy distribution over time (left-to-right) and across continents (up-and-down). It makes it easy to see that Africa has spread it’s life expectancy distribution (some improved, some didn’t), while Europe has become more clustered at the top end over time. Faceting in a grid is very helpful when you have a time dimension.\n\n\nVisual cues to be compared should be adjacent, continued\nFor each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.\nNote that there are two gapminder datasets, one in dslabs and one in the gapminder package. The dslabs version has more data, so I will switch to that here by using dslabs::gapminder as our data.\n\ndslabs::gapminder %>%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %>%\n  mutate(dollars_per_day = gdp/population/365) %>%\n  mutate(labels = paste(year, continent)) %>%  # creating text labels\n  ggplot(aes(x = labels, y = dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\")\n\n\n\n\n\n\n\n\nThe default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:\n\ndslabs::gapminder %>%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %>%\n  mutate(dollars_per_day = gdp/population/365) %>%\n  mutate(labels = paste(continent, year)) %>%\n  ggplot(aes(labels, dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\") + xlab('Continent and Year') \n\n\n\n\n\n\n\n\n\n\nLeave some space\nThe design maven Edward Tufte emphasizes th eneed for clarifying and empty space. Resist the urge to pack everything into a small layout. Especially in digital formats, space can be inexpensive, and can help attract the eye to your work.\nWe can control the space around our plots using the theme() function. We’ll cover more details of theme() on Thursday. To add some space around a plot, we use theme(plot.margin = margin(t=2, r = 2, b = 2, l = 2, unit = 'cm')), where the arguments correspond to top, right, bottom, left (in that order). I’m also going to add a black border to the outside so that we can see the boundary of the frame.\n\ndslabs::gapminder %>%\n  filter(year %in% c(1970, 2010) & !is.na(gdp)) %>%\n  mutate(dollars_per_day = gdp/population/365) %>%\n  mutate(labels = paste(continent, year)) %>%\n  ggplot(aes(labels, dollars_per_day)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25)) +\n  scale_y_continuous(trans = \"log2\") +\n  ylab(\"Income in dollars per day\") + xlab('Continent and Year') +\n  theme(plot.margin = margin(t=2, r = 2, b = 2, l = 2, unit = 'cm'),\n        plot.background = element_rect(color = 'black', size = 1)) # Adds black border\n\n\n\n\n\n\n\n\n\n\nUse color\nThe comparison becomes even easier to make if we use color to denote the two things we want to compare. Now we do not have to make the labels column and can just use continent on the x-axis:"
  },
  {
    "objectID": "02-content.html#think-of-the-color-blind",
    "href": "02-content.html#think-of-the-color-blind",
    "title": "Effective Visualizations",
    "section": "Think of the color blind",
    "text": "Think of the color blind\nAbout 10% of the population is color blind. Unfortunately, the default colors used in ggplot2 are not optimal for this group. However, ggplot2 does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette:\n\ncolor_blind_friendly_cols <-\n  c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n    \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nHere are the colors\n\n\n\n\n\n\n\n\n\nFrom Seafood Prices Reveal Impacts of a Major Ecological Disturbance: \nThere are several resources that can help you select colors, for example this one: http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/.\n\nUsing a discrete color palette\nIf you’re simply trying to differentiate between groups by using color, there are many ways of changing your color palette in ggplot. Most use scale_fill_discrete or scale_color_discrete (depending on the aesthetic for which you’re setting the color).\nThe easiest way of getting good-looking (e.g. non-default) colors is the scale_fill_viridis_d function, which “inherits” (takes the place of and has the properties of) scale_fill_discrete. Viridis has four color palettes and each is designed to be used to maximize the differentiation between colors.\nWe will subset our dslabs::gapminder dataset to five different years and take a look at what Viridis colors can do across those five:\n\ngp = dslabs::gapminder %>% \nfilter(year == 1990 | year == 1995 | year==2000 |  year == 2005 | year==2010 ) %>%\nggplot(aes(x = continent, y = gdp/population, fill = as.factor(year)))  + coord_flip()\n\ngp + geom_boxplot()  + labs(title = 'Default')\n\n\n\n\n\n\n\n\nThe default uses five different colors plucked seemingly at random. They are actually drawn from a palette of default ggplot colors.\nLet’s try Viridis\n\ngp = dslabs::gapminder %>% \nfilter(year == 1990 | year == 1995 | year==2000 |  year == 2005 | year==2010 ) %>%\nggplot(aes(x = continent, y = gdp/population, fill = as.factor(year)))  + coord_flip() + labs(fill = 'Year')\n\nviridis_a = gp + geom_boxplot()  + labs(title = 'Viridis A') + scale_fill_viridis_d(option = 'A')\nviridis_b = gp + geom_boxplot()  + labs(title = 'Viridis B') + scale_fill_viridis_d(option = 'B')\nviridis_c = gp + geom_boxplot()  + labs(title = 'Viridis C') + scale_fill_viridis_d(option = 'C')\nviridis_d = gp + geom_boxplot()  + labs(title = 'Viridis D') + scale_fill_viridis_d(option = 'D')\n\ngrid.arrange(viridis_a, viridis_b, viridis_c, viridis_d)\n\n\n\n\n\n\n\n\nViridis uses a better palette of colors that, though distinct, have some cohesiveness to them.\nWe can also use a custom palette, like the colorblind palette from before (see above where we defined color_blind_friendly_cols). We just need to use the type= argument and give ggplot our color-blind friendly palette of colors. If the palette has more entries than we have (N) distinct categories, R reverts to the default.\n\ngp = dslabs::gapminder %>% \nfilter(year == 1990 | year == 1995 | year==2000 |  year == 2005 | year==2010 ) %>%\nggplot(aes(x = continent, y = gdp/population, fill = as.factor(year)))  + coord_flip() + labs(fill = 'Year') \n\ncustom_a = gp + geom_boxplot()  + labs(title = 'Custom palette') + scale_fill_discrete(type = color_blind_friendly_cols)\ncustom_b = gp + geom_boxplot()  + labs(title = 'Custom palette 1-3') + scale_fill_discrete(type = color_blind_friendly_cols[1:3])\n\ngrid.arrange(custom_a, custom_b)\n\n\n\n\n\n\n\n\nIn the lower plot, we only give it a length-3 vector of colors, and it needs 5, so it returns to default. Unfortunately, it doesn’t warn you as to this behavior. Bad R!\n\n\nUsing a continuous color palette\nWe may often want to use the color to indicate a numeric value instead of simply using it to delineate groupings. When this is the case, the fill or color aesthetic is set to a continuous value. For instance, if one were to plot election results by precinct, we may represent precincts with heavy Republican support as dark red, swing districts as purple or white, and Democratic districts as blue. The intensity of red/blue indicates how heavily slanted votes in that precinct were in the election. This is known as a color ramp.\nLets plot one country’s GDP by year, but have the color indicate the life expectancy. Whenever you map a continuous variable to a color or fill aesthetic, you get the default color ramp – dark-to-light blue:\n\ndslabs::gapminder %>%\n  filter(country=='Romania' & year>1980) %>%\n  ggplot(aes(x = year, y = gdp/population, color = life_expectancy)) + \n  geom_point(size = 5) +\n  labs(x = 'Year', y = 'GDP Per Capita', fill = 'Life Expectancy')\n\n\n\n\n\n\n\n\nWe can see that GDP per capita went up, then down in 1989 (fall of the Soviet Union), then up after that. The color ramp tells us that life expectancy reached 75 years near the end, and it certainly improved in the post-2000 era.\nWe can set some of the points on the ramp manually - here, the ramp starts at dark blue and ends at light blue, but what if we wanted to start at red, and at blue, and cross white in the middle? Easy! We use scale_color_gradient2 and specify the colors for low, mid, and high, and specify the midpoint at 72.5 years.\n\ndslabs::gapminder %>%\n  filter(country=='Romania' & year>1980) %>%\n  ggplot(aes(x = year, y = gdp/population, color = life_expectancy)) + \n  scale_color_gradient2(low = 'red', mid = 'white', high = 'blue', midpoint = 72.5) + \n  geom_point(size = 5) +\n  labs(x = 'Year', y = 'GDP Per Capita', color = 'Life Expectancy')\n\n\n\n\n\n\n\n\nThe midpoint specification is extra useful when there is a threshold (like 50% of the vote) that indicates a different qualitative outcome.\nThe gradient2 method does not always work with the colorblind discrete palette - the colors interpolated may be in the range in which colorblindness tends to be a problem:\n\ndslabs::gapminder %>%\n  filter(country=='Romania' & year>1980) %>%\n  ggplot(aes(x = year, y = gdp/population, color = life_expectancy)) + \n  scale_color_gradient2(low = color_blind_friendly_cols[3], mid = color_blind_friendly_cols[4], high = color_blind_friendly_cols[5], midpoint = 72.5) + \n  geom_point(size = 5) +\n  labs(x = 'Year', y = 'GDP Per Capita', color = 'Life Expectancy')"
  },
  {
    "objectID": "02-content.html#gridextra-and-grid.arrange",
    "href": "02-content.html#gridextra-and-grid.arrange",
    "title": "Effective Visualizations",
    "section": "gridExtra and grid.arrange",
    "text": "gridExtra and grid.arrange\nThe gridExtra package has been used a few times in this lesson to combine plots using the grid.arrange function. The use is pretty intuitive - you save your plots as objects plot1 <- ggplot(data, aes(x = var1)) and plot2 <- ggplot(data, aes(x = var2)), and then use grid.arrange(plot1, plot2) to combine. The function will align as best it can, and there are more advanced grob-based functions that can adjust and align axes between plots, but we won’t get into them. If we want to set the layout, we can specify nrow and ncol to set the rows and columns.\nThe very-useful patchwork package is quickly replacing grid.arrange and provides more flexibility."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About page\n\n\n\nThis page contains some elaborated background information about your workshop, or the instructors.\n\n\nFor example: A central problem in machine learning is how to make an algorithm perform well not just on the training data, but also on new inputs. Many strategies in machine learning are explicitly designed to reduce this test error, possibly at the expense of increased training error. These strategies are collectively known as regularisation and they are instrumental for good performance of any kind of prediction or classification model, especially in the context of small data (many features, few samples).\nIn the hands-on tutorial we will use R to perform an integrated analysis of multi-omics data with penalised regression.\n\nContact\nInstructor A: contact\nInstructor B: contact\nInstructor C: contact"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "Date: Date and time\nRoom: Location\nInstructors: Instructor A, B, C\n\n\n\n\n\n\n\nThis workshop template\n\n\n\nThis workshop template contains 4 pages:\n\nHome: index.qmd (this page)\nAbout: about.qmd\n\nTwo content pages\n\nPage without code: part_1_prep.qmd\nPage with R code: part_2_eda.qmd\n\nIt is straightforward to add more content pages: you need to create a new .qmd file (or copy/paste the existing ones), then link the new page inside _quarto.yml.\n\n\n\n\n\n\n\n\nHomepage of your workshop\n\n\n\nThis is the homepage index.qmd for your workshop, so ideally it should contain some key information such as time and place, instructors and information of the course/workshop.\nIt should be easy to navigate.\n\n\n\nWelcome!\n\nThe goal of the workshop is to … (insert your message)\nFor example, introduce kep concepts in machine learning, such as regularisation.\nWorkshop material can be found in the workshop github repository.\n\n\nLearning Objectives\nAt the end of the tutorial, participants will be able to\n\nunderstand key concepts for … (insert your message)\nFor example, training machine learning models such as regularisation.\nanother objective\n\n\n\nPre-requisites\n\nBasic familiarity with R\nSome other knowledge\n\n\n\n\nSchedule\n\n\n\n\n\n\nTabular schedule\n\n\n\nIt can be useful to include a tabular schedule with links.\n\n\n\n\n\nTime\nTopic\nPresenter\n\n\n\n\n9:00 - 10:30\nSession 1: Preparation\nInstructor A\n\n\n10:45 - 12:00\nSession 2: Exploratory Analysis\nInstructor B, C"
  },
  {
    "objectID": "part_1_prep.html",
    "href": "part_1_prep.html",
    "title": "Preparation",
    "section": "",
    "text": "Page without code\n\n\n\nThis page contains an example for some structured preparation information for a workshop. No code is executed here.\nHere are some preparation information for the participants."
  },
  {
    "objectID": "part_1_prep.html#software",
    "href": "part_1_prep.html#software",
    "title": "Preparation",
    "section": "Software",
    "text": "Software\nIn this workshop we will be using R. You can either\n\nhave R and Rstudio installed on your laptop\nor, use Posit cloud (formerly Rstudio Cloud).\n\nPosit cloud is free of charge for personal users, yet you need to sign up for a new user account and have internet connection.\nThe R package we are using is glmnet."
  },
  {
    "objectID": "part_1_prep.html#data",
    "href": "part_1_prep.html#data",
    "title": "Preparation",
    "section": "Data",
    "text": "Data\nThe datasets we use can be found here (insert link)."
  },
  {
    "objectID": "part_1_prep.html#code",
    "href": "part_1_prep.html#code",
    "title": "Preparation",
    "section": "Code",
    "text": "Code\nThe R scripts used in part 1 and part 2 can be found here (insert link)."
  },
  {
    "objectID": "part_1_prep.html#resources",
    "href": "part_1_prep.html#resources",
    "title": "Preparation",
    "section": "Resources",
    "text": "Resources\nLecture notes (insert link)\nLab notes (insert link)"
  },
  {
    "objectID": "part_2_eda.html",
    "href": "part_2_eda.html",
    "title": "Part I",
    "section": "",
    "text": "Page with R code\n\n\n\nThis page contains an example template for a lab session, where R code and results are displayed here.\nYou can find more information on how to include code in Quarto website here.\nYou can experiment with code-fold and code-tools in the yaml header above to change how the code cells look like."
  },
  {
    "objectID": "part_2_eda.html#a-cancer-modeling-example",
    "href": "part_2_eda.html#a-cancer-modeling-example",
    "title": "Part I",
    "section": "A Cancer Modeling Example",
    "text": "A Cancer Modeling Example\nExercise on analysis of miRNA, mRNA and protein data from the paper Aure et al, Integrated analysis reveals microRNA networks coordinately expressed with key proteins in breast cancer, Genome Medicine, 2015.\nPlease run the code provided to replicate some of the analyses. Make sure you can explain what all the analysis steps do and that you understand all the results.\nIn addition, there are some extra tasks (Task 1), where no R code is provided. Please do these tasks when you have time available at the end of the lab.\n\nLoad the data\nRead the data, and convert to matrix format.\n\nmrna <- read.table(\"data/data_example.txt\", header=T, sep=\"\\t\", dec=\".\")\n\n# Convert to matrix format\n\nmrna <- as.matrix(mrna)\n\nPrint the data\n\nmrna[1:4, 1:4]\n\n      OSL2R.3002T4 OSL2R.3005T1 OSL2R.3013T1 OSL2R.3030T2\nACACA      1.60034     -0.49087     -0.26553     -0.27857\nANXA1     -2.42501     -0.05416     -0.46478     -2.18393\nAR         0.39615     -0.43348     -0.10232      0.58299\nBAK1       0.78627      0.39897      0.22598     -1.31202\n\n\nVisualise the overall distribution of expression levels by histogram\n\nhist(mrna, nclass=40, xlim=c(-5,5), col=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\nTask 1\n\n\n\nThis is a callout-note, and it can be quite useful for exercises. You can find more about callout here.\nExample: Extend the above analysis to cover all genes."
  }
]